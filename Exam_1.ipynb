{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exam_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN97XJe58UxJ3Yux8OwoQ5d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armeldjogo/NLP/blob/main/Exam_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0LNTa2GYL4Z"
      },
      "source": [
        "Name: Armel Nsiangani\r\n",
        "\r\n",
        "Student ID#: 001-99-1988\r\n",
        "\r\n",
        "\r\n",
        "NLP - Exam 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ-Xp08v-ZGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b95af19-3fdd-4f59-c81e-a3fbc7d23863"
      },
      "source": [
        "# Downloading packages\r\n",
        "%matplotlib inline\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns; sns.set() \r\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Bag of words\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # TF-IDF\r\n",
        "from sklearn.naive_bayes import MultinomialNB # Bayes\r\n",
        "from sklearn.ensemble import RandomForestClassifier # RF\r\n",
        "from sklearn.svm import SVC # SVC\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import sklearn.metrics \r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import sklearn.metrics \r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze5e_gBXYT0-"
      },
      "source": [
        "Question 1: \r\n",
        "\r\n",
        "Write a generic function that takes: Classification algorithm name, vectorization method name, training set with labels as parameters (total of 3 parameters should be passed). The function should take the classification algorithm name, the vectorization method’s name, and the training set and train the desired model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBdZrM60-VG_"
      },
      "source": [
        "# Question 1\r\n",
        "# Unzip files uploaded\r\n",
        "import zipfile\r\n",
        "path_to_zip_file = \"/content/exam1_dataset.zip\"\r\n",
        "directory_to_extract_to = \"/content/\"\r\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\r\n",
        "    zip_ref.extractall(directory_to_extract_to) \r\n",
        "\r\n",
        "# Create Training data - negative dataSet\r\n",
        "import glob, os\r\n",
        "dirName_1 = \"/content/exam1_dataset/TRAINING/negative/\"\r\n",
        "dirName_2 = \"/content/exam1_dataset/TRAINING/positive/\"\r\n",
        "dirName_3 = \"/content/exam1_dataset/UNLABELED/\"\r\n",
        "\r\n",
        "def returnDataInArray(dirName):\r\n",
        "  os.chdir(dirName)\r\n",
        "  filesName = []\r\n",
        "  for namefile in glob.glob(\"*.txt\"):\r\n",
        "    filesName.append(namefile)\r\n",
        "\r\n",
        "  reviews = []\r\n",
        "  for i in range(0,len(filesName)):\r\n",
        "    with open(dirName + filesName[i]) as openfile:\r\n",
        "      data = openfile.read()\r\n",
        "      reviews.append(data) \r\n",
        "  return reviews,filesName\r\n",
        "\r\n",
        "\r\n",
        "negReviews,neg_fileName = returnDataInArray(dirName_1)\r\n",
        "posReviews,pos_fileName = returnDataInArray(dirName_2)\r\n",
        "unlabeledReviews,ulb_fileName = returnDataInArray(dirName_3)\r\n",
        "\r\n",
        "# Lets merge neg & pos reviews by altenating b/w the sets \r\n",
        "mergedReviews = []\r\n",
        "targetReviews = []\r\n",
        "for i in range(0,len(negReviews)):\r\n",
        "    mergedReviews.append(negReviews[i])\r\n",
        "    mergedReviews.append(posReviews[i])\r\n",
        "    targetReviews.append(0)\r\n",
        "    targetReviews.append(1)\r\n",
        "\r\n",
        "\r\n",
        "# Function to return models\r\n",
        "def createModels(classAlgName,vectorMethod,trainingSet):\r\n",
        "  # create model\r\n",
        "  model = make_pipeline(vectorMethod, classAlgName)\r\n",
        "  # fit model\r\n",
        "  model.fit(trainingSet['data'], trainingSet[['target']].values)\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4L-JKqn91q7"
      },
      "source": [
        "Question 2: Using the function from question 1 to build the following models:\r\n",
        "\r\n",
        "a) Model a: Naive Bayes, Vectorizer: TFIDF and Bag of Words, Training set should be 75% of the provided dataset. Leaving the remaining 25% for testing.\r\n",
        "\r\n",
        "b) Model b: RandomForest, Vectorizer: TFIDF and Bag of Words, Training set should be 70% of the provided dataset. Leaving the remaining 30% for testing.\r\n",
        "\r\n",
        "c) Model c: Support Vector Machines (SVC in sklearn), Vectorizer: TFIDF and Bag of Words, Training set should be 60% of the provided dataset. Leaving the remaining 40% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buXXS48X7G5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4f31f9-c5b1-40cb-de89-cf36c1b39029"
      },
      "source": [
        "# Question 2\r\n",
        "from numpy.random import seed\r\n",
        "seed(12345)\r\n",
        "\r\n",
        "# DataFrame with merged data\r\n",
        "dF = pd.DataFrame({\"data\": mergedReviews,\"target\": targetReviews})\r\n",
        "\r\n",
        "# Bayes\r\n",
        "Btrain, Btest = train_test_split(dF, train_size= 0.75, test_size = 0.25)\r\n",
        "\r\n",
        "modelBayes_TDIDF = createModels(MultinomialNB(),TfidfVectorizer(),Btrain)\r\n",
        "modelBayes_BOW = createModels(MultinomialNB(),CountVectorizer(),Btrain) \r\n",
        "\r\n",
        "# RF\r\n",
        "RFtrain, RFtest = train_test_split(dF, train_size= 0.70, test_size = 0.30)\r\n",
        "\r\n",
        "modelRF_TDIDF = createModels(RandomForestClassifier(),TfidfVectorizer(),RFtrain)\r\n",
        "modelRF_BOW = createModels(RandomForestClassifier(),CountVectorizer(),RFtrain)\r\n",
        "\r\n",
        "# SVC\r\n",
        "SVCtrain, SVCtest = train_test_split(dF, train_size= 0.60, test_size = 0.40)\r\n",
        "\r\n",
        "modelSVC_TDIDF = createModels(SVC(),TfidfVectorizer(),SVCtrain)\r\n",
        "modelSVC_BOW = createModels(SVC(),CountVectorizer(),SVCtrain)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:354: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self._final_estimator.fit(Xt, y, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:354: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self._final_estimator.fit(Xt, y, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljEcJb472L2"
      },
      "source": [
        "Question 3:\r\n",
        "\r\n",
        "Using the models from Question 2, evaluate each model with its\r\n",
        "respective training set (for model a, that set is 25% of the data, for model b, that set is 30% of\r\n",
        "the data, and for model c that set is 40% of the data. Be careful to not mix up the evaluation\r\n",
        "sets. With the predictions on the test set and show the following metrics: Accuracy, Precision,\r\n",
        "Recall, and Macro F1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAg4qa938TB0",
        "outputId": "bec3d2d8-f89d-469e-8745-fbc2f57e8181"
      },
      "source": [
        "# Question 3\r\n",
        "# Bayes\r\n",
        "labels_B1 = modelBayes_TDIDF.predict(Btest['data'])\r\n",
        "labels_B2 = modelBayes_BOW.predict(Btest['data'])\r\n",
        "mat_B1 = confusion_matrix(Btest[['target']], labels_B1)\r\n",
        "mat_B2 = confusion_matrix(Btest[['target']], labels_B2)\r\n",
        "\r\n",
        "print('Results using Bayes & TD-IDF')\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(Btest[['target']],labels_B1))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(Btest[['target']],labels_B1))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(Btest[['target']],labels_B1))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_B1, Btest[['target']], average='macro'))\r\n",
        "\r\n",
        "print()\r\n",
        "print('Results using Bayes & BOW')\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(Btest[['target']],labels_B2))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(Btest[['target']],labels_B2))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(Btest[['target']],labels_B2))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_B2, Btest[['target']], average='macro'))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results using Bayes & TD-IF\n",
            "Accuracy: 0.86128\n",
            "Precision: 0.8902564102564102\n",
            "Recall: 0.8266666666666667\n",
            "F1 Score: 0.8611711581880195\n",
            "\n",
            "Results using Bayes & BOW\n",
            "Accuracy: 0.85024\n",
            "Precision: 0.876786929884275\n",
            "Recall: 0.8177777777777778\n",
            "F1 Score: 0.8501393166393827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwmNssc7_Vyq",
        "outputId": "f100bc31-336b-4440-b141-49ad326258d9"
      },
      "source": [
        "# Random Forest\r\n",
        "labels_RF1 = modelRF_TDIDF.predict(RFtest['data'])\r\n",
        "labels_RF2 = modelRF_BOW.predict(RFtest['data'])\r\n",
        "mat_RF1 = confusion_matrix(RFtest[['target']], labels_RF1)\r\n",
        "mat_RF2 = confusion_matrix(RFtest[['target']], labels_RF2)\r\n",
        "\r\n",
        "print('Results using Random Forest & TD-IDF')\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(RFtest[['target']],labels_RF1))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(RFtest[['target']],labels_RF1))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(RFtest[['target']],labels_RF1))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_RF1, RFtest[['target']], average='macro'))\r\n",
        "\r\n",
        "print()\r\n",
        "print('Results using Random Forest & BOW')\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(RFtest[['target']],labels_RF2))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(RFtest[['target']],labels_RF2))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(RFtest[['target']],labels_RF2))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_RF2, RFtest[['target']], average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results using Random Forest & TD-IF\n",
            "Accuracy: 0.8317333333333333\n",
            "Precision: 0.8372410032715376\n",
            "Recall: 0.8220021413276232\n",
            "F1 Score: 0.8317057600050526\n",
            "\n",
            "Results using Random Forest & BOW\n",
            "Accuracy: 0.8338666666666666\n",
            "Precision: 0.8304140127388535\n",
            "Recall: 0.8375267665952891\n",
            "F1 Score: 0.8338666194109494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsDS3KLMApP1",
        "outputId": "cf57e939-b8f2-472c-d3de-ee4e708aad85"
      },
      "source": [
        "# SVC\r\n",
        "labels_SVC1 = modelSVC_TDIDF.predict(SVCtest['data'])\r\n",
        "labels_SVC2 = modelSVC_BOW.predict(SVCtest['data'])\r\n",
        "mat_SVC1 = confusion_matrix(SVCtest[['target']], labels_SVC1)\r\n",
        "mat_SVC2 = confusion_matrix(SVCtest[['target']], labels_SVC2)\r\n",
        "\r\n",
        "print('Results using Random Forest & TD-IDF')\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(SVCtest[['target']],labels_SVC1))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(SVCtest[['target']],labels_SVC1))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(SVCtest[['target']],labels_SVC1))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_SVC1, SVCtest[['target']], average='macro'))\r\n",
        "\r\n",
        "print()\r\n",
        "print('Results using Random Forest & BOW')\r\n",
        "print('Accuracy:', sklearn.metrics.accuracy_score(SVCtest[['target']],labels_SVC2))\r\n",
        "print('Precision:', sklearn.metrics.precision_score(SVCtest[['target']],labels_SVC2))\r\n",
        "print('Recall:', sklearn.metrics.recall_score(SVCtest[['target']],labels_SVC2))\r\n",
        "print('F1 Score:', sklearn.metrics.f1_score(labels_SVC2, SVCtest[['target']], average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results using Random Forest & TD-IF\n",
            "Accuracy: 0.893\n",
            "Precision: 0.8829849579996093\n",
            "Recall: 0.905630134241635\n",
            "F1 Score: 0.8929870514332234\n",
            "\n",
            "Results using Random Forest & BOW\n",
            "Accuracy: 0.8542\n",
            "Precision: 0.8332390115072628\n",
            "Recall: 0.884992987377279\n",
            "F1 Score: 0.85407557900168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4FIQKsZ_B7K"
      },
      "source": [
        "a) What model performs the best?\r\n",
        "\r\n",
        "The best performing model is SVC performed with TD-IDF. This is because this model has the highest accuracy, recall and f1_score. In addition, it has the second largest precision score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6gw0H0AOOA"
      },
      "source": [
        "b) Why is it important not to mix up the testing sets between different models?\r\n",
        "\r\n",
        "Mixing up testing sets between different models can lead to misleading results affecting the prediction ability of the model. It is important to separate the testing sets between models as it can lead to unexpected dependancy or bias.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzBpLteKBcZ4"
      },
      "source": [
        "c) Display in a single sorted dataframe (model name, training %, test %, accuracy,\r\n",
        "precision, recall, F1-score) all performance metrics, sorted by accuracy in descending\r\n",
        "manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsUSLQnQBq6W",
        "outputId": "cd5f5e6e-376e-46f1-b803-a2f15ace518d"
      },
      "source": [
        "model_name = [\"SVC + TD-IDF\",\"Bayes + TD-IDF\",\"SVC + BOW\",\"Bayes + BOW\",\"RF + BOW\",\"RF + TD-IDF\"]\r\n",
        "training_perc = [60,75,60,75,70,70]\r\n",
        "test_perc = [40,25,40,25,30,30]\r\n",
        "accuracy = [sklearn.metrics.accuracy_score(SVCtest[['target']],labels_SVC1),\r\n",
        "            sklearn.metrics.accuracy_score(Btest[['target']],labels_B1),\r\n",
        "            sklearn.metrics.accuracy_score(SVCtest[['target']],labels_SVC2),\r\n",
        "            sklearn.metrics.accuracy_score(Btest[['target']],labels_B2),\r\n",
        "            sklearn.metrics.accuracy_score(RFtest[['target']],labels_RF2),\r\n",
        "            sklearn.metrics.accuracy_score(RFtest[['target']],labels_RF1)]\r\n",
        "\r\n",
        "precision = [sklearn.metrics.precision_score(SVCtest[['target']],labels_SVC1),\r\n",
        "            sklearn.metrics.precision_score(Btest[['target']],labels_B1),\r\n",
        "            sklearn.metrics.precision_score(SVCtest[['target']],labels_SVC2),\r\n",
        "            sklearn.metrics.precision_score(Btest[['target']],labels_B2),\r\n",
        "            sklearn.metrics.precision_score(RFtest[['target']],labels_RF2),\r\n",
        "            sklearn.metrics.precision_score(RFtest[['target']],labels_RF1)]\r\n",
        "\r\n",
        "recall = [sklearn.metrics.recall_score(SVCtest[['target']],labels_SVC1),\r\n",
        "            sklearn.metrics.recall_score(Btest[['target']],labels_B1),\r\n",
        "            sklearn.metrics.recall_score(SVCtest[['target']],labels_SVC2),\r\n",
        "            sklearn.metrics.recall_score(Btest[['target']],labels_B2),\r\n",
        "            sklearn.metrics.recall_score(RFtest[['target']],labels_RF2),\r\n",
        "            sklearn.metrics.recall_score(RFtest[['target']],labels_RF1)]\r\n",
        "\r\n",
        "f1_score = [sklearn.metrics.f1_score(SVCtest[['target']],labels_SVC1),\r\n",
        "            sklearn.metrics.f1_score(Btest[['target']],labels_B1),\r\n",
        "            sklearn.metrics.f1_score(SVCtest[['target']],labels_SVC2),\r\n",
        "            sklearn.metrics.f1_score(Btest[['target']],labels_B2),\r\n",
        "            sklearn.metrics.f1_score(RFtest[['target']],labels_RF2),\r\n",
        "            sklearn.metrics.f1_score(RFtest[['target']],labels_RF1)]\r\n",
        "\r\n",
        "# Data Frame + scores\r\n",
        "dataFrame_with_scores = pd.DataFrame({\"model name\": model_name,\"training %\": training_perc, \"test %\": test_perc,\r\n",
        "                                      \"accuracy\" : accuracy, \"precision\": precision,\r\n",
        "                                      \"recall\":recall, \"f1_score\":f1_score})\r\n",
        "print(dataFrame_with_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       model name  training %  test %  accuracy  precision    recall  f1_score\n",
            "0    SVC + TD-IDF          60      40  0.893000   0.882985  0.905630  0.894164\n",
            "1  Bayes + TD-IDF          75      25  0.861280   0.890256  0.826667  0.857284\n",
            "2       SVC + BOW          60      40  0.854200   0.833239  0.884993  0.858337\n",
            "3     Bayes + BOW          75      25  0.850240   0.876787  0.817778  0.846255\n",
            "4        RF + BOW          70      30  0.833867   0.830414  0.837527  0.833955\n",
            "5     RF + TD-IDF          70      30  0.831733   0.837241  0.822002  0.829552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIr59Vd9BxE-"
      },
      "source": [
        "Question 4:\r\n",
        "\r\n",
        "Using the documents in the folder named UNLABELED, please use\r\n",
        "your best performing trained model from question 3 to predict their labels. Please do this\r\n",
        "individually for each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccItIFzaHdiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a885917-afab-4f56-97b0-5cb466379d1d"
      },
      "source": [
        "# Question 4\r\n",
        "# Using SVC + TD-IDF to predict\r\n",
        "def predict_category(s, model = modelSVC_TDIDF):\r\n",
        "    pred = model.predict([s])\r\n",
        "    if pred[0] == 0:\r\n",
        "      f_pred = 'negative'\r\n",
        "    else:\r\n",
        "      f_pred = 'positive'\r\n",
        "    return f_pred,pred\r\n",
        "\r\n",
        "#unlabeledReviews,ulb_fileName\r\n",
        "predictions = []\r\n",
        "score = []\r\n",
        "for k in range(0,len(unlabeledReviews)):\r\n",
        "  x,scr = predict_category(unlabeledReviews[k])\r\n",
        "  predictions.append(x)\r\n",
        "  score.append(scr)\r\n",
        "\r\n",
        "\r\n",
        "dataFrame_pred = pd.DataFrame({\"Doc name\": ulb_fileName, \"Predicted labeled\": predictions})\r\n",
        "print(dataFrame_pred)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Doc name Predicted labeled\n",
            "0   46278_0.txt          negative\n",
            "1   36517_0.txt          negative\n",
            "2   24221_0.txt          negative\n",
            "3   46705_0.txt          negative\n",
            "4   37154_0.txt          negative\n",
            "5       0_0.txt          positive\n",
            "6   36022_0.txt          negative\n",
            "7   36149_0.txt          negative\n",
            "8   35991_0.txt          negative\n",
            "9   49990_0.txt          negative\n",
            "10  35968_0.txt          negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_TX_4-mERcF"
      },
      "source": [
        "Own opinion: \r\n",
        "\r\n",
        "\r\n",
        "I have to admit, that I was a little surprised when the model predicted that all reviews except for the first one was negative. However, after reading the review, I believe the model was mostly right. Two mistakes may be with the prediction of review 0_0.txt and 46278_0.txt. I think that 0_0.txt is negative, and 46278_0.txt is positive, but I may be wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxJI5LezQGOU"
      },
      "source": [
        "# Question 5\r\n",
        "def matrixSimCos(all_docs):\r\n",
        "  vect = TfidfVectorizer(stop_words = 'english')\r\n",
        "  tfidf_row = vect.fit_transform(all_docs)                                                                                                                                                                                                \r\n",
        "  tfidf = vect.transform(all_docs)                                                                                                                                                                                                                       \r\n",
        "  cos_similarity = sklearn.metrics.pairwise.cosine_similarity(tfidf,tfidf_row)\r\n",
        "  return cos_similarity\r\n",
        "\r\n",
        "alldocs = negReviews + posReviews\r\n",
        "rslt = matrixSimCos(alldocs)\r\n",
        "\r\n",
        "sns.heatmap(rslt, square=True, annot=False, cbar=True,\r\n",
        "            xticklabels= \"Negative to Positive reviews\", yticklabels= \"Negative to Positive reviews\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdujuScnX-rY"
      },
      "source": [
        "# Question 6\r\n",
        "def findMostSim(m,n):\r\n",
        "  array = np.around(m, decimals=3)\r\n",
        "  rArray = array.reshape(-1)\r\n",
        "  uN_Array = np.unique(rArray)\r\n",
        "  listVal = np.flip(uN_Array)\r\n",
        "  listVal = np.delete(listVal, np.where(listVal == 1))\r\n",
        "  valsOfInterest = listVal[0:n]\r\n",
        "  locVals = []\r\n",
        "  for i in range(0,len(valsOfInterest)):\r\n",
        "    rslt = np.where(array == valsOfInterest[i])\r\n",
        "    locVals.append(rslt[0].tolist())\r\n",
        "\r\n",
        "  return locVals,valsOfInterest\r\n",
        "\r\n",
        "m = rslt\r\n",
        "n = 50\r\n",
        "filesName = neg_fileName + pos_fileName\r\n",
        "loc,vals = findMostSim(m,n)\r\n",
        "print(\"The\",n,\"most similar works are in position: \")\r\n",
        "print('-----------------------------------------')\r\n",
        "indx = 0\r\n",
        "for pos in loc:\r\n",
        "  print(filesName[pos[0]],'and',filesName[pos[1]], 'Similarity = ', vals[indx])\r\n",
        "  indx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3IC33pWFwwA"
      },
      "source": [
        "Do all similar documents belong to same class? \r\n",
        "\r\n",
        "No, all similar documents do not belong to same class. Exisiting similarities were found between Negative and Positive reviews. This is probably due to people who somwhat liked or somewhat disliked the movie. Since, their sentiments were not strong enough to fully put in one category, there will be a high probability of similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGPj_SVV8ekO"
      },
      "source": [
        "Question 7:\r\n",
        "\r\n",
        "\r\n",
        "Using Spacy’s part of speech tagger, process all sentences (hint: don’t\r\n",
        "forget to split the reviews) and count how many NOUN and VERB tags are found in all the\r\n",
        "movies review (TRAINING folder) separating them by label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BryIoq-uedyA",
        "outputId": "4a7b7455-1def-4f4e-d653-b29da4181615"
      },
      "source": [
        "# Question 7\r\n",
        "import spacy\r\n",
        "nlp = spacy.load(\"en_core_web_sm\")\r\n",
        "def foundCount(docSet):\r\n",
        "  verb_count = 0\r\n",
        "  noun_count = 0\r\n",
        "  for kk in range(0,len(docSet)):\r\n",
        "    doc_1 = nlp(docSet[kk])\r\n",
        "    for token in doc_1:\r\n",
        "      if token.pos_ == 'NOUN':\r\n",
        "        noun_count += 1\r\n",
        "      elif token.pos_ == 'VERB':\r\n",
        "        verb_count += 1\r\n",
        "  return verb_count,noun_count\r\n",
        "\r\n",
        "neg_verb, neg_noun = foundCount(negReviews)\r\n",
        "pos_verb, pos_noun = foundCount(posReviews)\r\n",
        "\r\n",
        "print(\"In Negative reviews, there are\",neg_noun, \"nouns and\", neg_verb, \"verbs\")\r\n",
        "print(\"In Positive reviews, there are\",pos_noun, \"nouns and\", pos_verb, \"verbs\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In Negative reviews, there are 528475 nouns and 355559 verbs\n",
            "In Positive reviews, there are 542978 nouns and 342125 verbs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM0_cdOBGFVj"
      },
      "source": [
        "Opinion:\r\n",
        "\r\n",
        "Yes, there is a difference between the number of nouns and verbs in Negative vs. Positive review. I notice that there is less nouns in Negative than Positive reviews, but less verbs in Positive then Negative reviews. However, in both cases, the difference in word counts is less than 10%, so I would not say that there is a huge disparity. This could mean that regardless of their impressions movies goers tend to express both positive and negative opinions equally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vls57B_p8lSS"
      },
      "source": [
        "Question 8:\r\n",
        "\r\n",
        "Using the results from the PoS process in question 7, count how many\r\n",
        "different PUNCT tags are found and their respective counts from the full dataset provided (both\r\n",
        "negative and positives together)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E1RANfmiwwV"
      },
      "source": [
        "# Question 8\r\n",
        "import re\r\n",
        "# Using Spacy\r\n",
        "def foundCount(docSet):\r\n",
        "  punct_count = 0\r\n",
        "  for kk in range(0,len(docSet)):\r\n",
        "    doc_1 = nlp(docSet[kk])\r\n",
        "    for token in doc_1:\r\n",
        "      if token.pos_ == 'PUNCT':\r\n",
        "        punct_count += 1\r\n",
        "  return punct_count\r\n",
        "spacy_punct_count = foundCount(mergedReviews)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKVCDB2YoTlO"
      },
      "source": [
        "# Using regex\r\n",
        "def punctCount(docSet):\r\n",
        "  data = docSet\r\n",
        "  punct_count = 0\r\n",
        "  for i in range(0,len(data)):\r\n",
        "    m = re.findall(r'[.?\\-/\",!]+', data[i])\r\n",
        "    punct_count = punct_count + len(m)\r\n",
        "  return punct_count\r\n",
        "regex_punct_count = punctCount(mergedReviews)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orzemBBeH-vy",
        "outputId": "0e94e1e8-5818-4a7d-bd29-b0b8a24b7bec"
      },
      "source": [
        "print('The number of PUNCT tags using Spacy is:',spacy_punct_count)\r\n",
        "print('The number of PUNCT tags using regex is:',regex_punct_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of PUNCT tags using Spacy is: 834967\n",
            "The number of PUNCT tags using regex is: 829922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL5MsbQCIBz5"
      },
      "source": [
        "Opinion:\r\n",
        "\r\n",
        "I was not able to get the same count using Spacy and Regex. This is probably because what I have defined as \"punctuation\" in my regex is not the same as the characters defined as \"punctuation\" by Spacy. However, if I used the same set of punctuation as Spacy, I would have had the same count."
      ]
    }
  ]
}